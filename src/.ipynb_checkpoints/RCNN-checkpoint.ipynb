{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zguo/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Dropout, Activation, \\\n",
    "    Flatten, Lambda, LSTM, RepeatVector, TimeDistributed, Reshape, \\\n",
    "    Conv2D, MaxPooling2D, BatchNormalization, ConvLSTM2D, Bidirectional, Masking\n",
    "import keras.callbacks as Callbacks \n",
    "import numpy as np\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.preprocessing import scale\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.util import random_noise\n",
    "from skimage.transform import rotate\n",
    "from scipy import ndimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data keys: ['images', 'labels', 'observation_days']\n"
     ]
    }
   ],
   "source": [
    "data = h5py.File(\"../../data_sample.hdf5\", \"r\")\n",
    "print(\"data keys: \" + str(list(data.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape: (sample, x_size, y_size, epoch) = (72000, 21, 21, 48)\n"
     ]
    }
   ],
   "source": [
    "images = data[\"images\"][:]\n",
    "print(\"image shape: (sample, x_size, y_size, epoch) = \" + str(images.shape))\n",
    "labels = data[\"labels\"][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_img(images, percent=0.25):\n",
    "    \n",
    "    size, timestep, _, _ = images.shape\n",
    "    sample_size = int(size * percent)\n",
    "    \n",
    "    np.random.seed(209)\n",
    "    # pick the sequence with replacement\n",
    "    sample_seq_indices = np.random.choice(np.arange(sample_size), sample_size)\n",
    "    # pick the timestep\n",
    "    sample_inseq_indices = np.random.choice(np.arange(timestep), sample_size)\n",
    "    # pick the aug mean\n",
    "    sample_aug_type = np.random.choice(np.arange(4), sample_size)\n",
    "    \n",
    "    for idx in range(sample_size):\n",
    "        i, j, k = sample_seq_indices[idx], \\\n",
    "            sample_inseq_indices[idx], sample_aug_type[idx]\n",
    "        if k == 0: # rotate\n",
    "            images[i, j] = rotate(images[i, j], 45)\n",
    "        elif k == 1: # random noise\n",
    "            images[i, j] = random_noise(images[i, j], seed=209)\n",
    "        elif k == 2: # horizontal flip\n",
    "            images[i, j] = images[i, j][:,::-1]\n",
    "        else: # vertical flip\n",
    "            images[i, j] = images[i, j][::-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_images(images):\n",
    "    t_images = np.transpose(images, (0,3,1,2))\n",
    "    rt_images = t_images.reshape(72000*48, 21, 21)\n",
    "    max_per_img = np.max(rt_images.reshape(-1, 21*21), axis=1, keepdims=1)\n",
    "    scaled_images = rt_images.reshape(-1, 21*21) / max_per_img\n",
    "    scaled_images = scaled_images.reshape(-1, 21, 21).reshape(-1, 48, 21, 21)\n",
    "    return scaled_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt2digit(labels):\n",
    "    dic = {'Asteroids':0, 'Constant':1, 'EmptyLigh':2, 'M33Cephei':3, 'RRLyrae':4, 'Supernova':5}\n",
    "    labels_digit = np.array([dic[i] for i in labels])\n",
    "    return labels_digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(images, labels):\n",
    "    scaled_img = scale_images(images)\n",
    "    preprocess_img(scaled_img)\n",
    "    x = np.expand_dims(scaled_img, len(scaled_img.shape))\n",
    "    y = to_categorical(txt2digit(labels))\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = build_dataset(images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(cnn_input_dim=21, cnn_output_dim=128, cnn_dropout=0.5,\n",
    "               rnn_hidden_dim=128, rnn_output_dim=64, num_classes=6, rnn_dropout=0.5, timestep=48):\n",
    "    \n",
    "    \n",
    "    intput_shape = (timestep, cnn_input_dim, cnn_input_dim, 1)\n",
    "    model = Sequential()\n",
    "    # CNN\n",
    "    model.add(TimeDistributed(Conv2D(32, (4,4), \\\n",
    "                     padding='same', activation='relu', kernel_initializer='uniform'), \\\n",
    "                              input_shape=intput_shape))\n",
    "    \n",
    "    model.add(BatchNormalization())\n",
    "    # model.add(Dropout(cnn_dropout))\n",
    "    \n",
    "    model.add(TimeDistributed(MaxPooling2D((3,3), strides=(1,1))))\n",
    "    model.add(TimeDistributed(Conv2D(64, (3,3), padding='same', activation='relu', kernel_initializer='uniform')))\n",
    "    \n",
    "    #model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(TimeDistributed(Conv2D(64, (3,3), padding='same', activation='relu', kernel_initializer='uniform')))\n",
    "    \n",
    "    #model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(TimeDistributed(MaxPooling2D((3,3), strides=(1,1))))\n",
    "    model.add(TimeDistributed(Conv2D(32, (3,3), padding='same', activation='relu', kernel_initializer='uniform')))\n",
    "    \n",
    "    #model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(TimeDistributed(Conv2D(32, (3,3), padding='same', activation='relu', kernel_initializer='uniform')))\n",
    "    \n",
    "    #model.add(BatchNormalization())\n",
    "#     model.add(Dropout(cnn_dropout))\n",
    "    \n",
    "    model.add(TimeDistributed(Conv2D(32, (3,3), padding='same', activation='relu', kernel_initializer='uniform')))\n",
    "    #model.add(Dropout(cnn_dropout))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(TimeDistributed(MaxPooling2D((3,3), strides=(1,1))))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    model.add(TimeDistributed(Dense(128, activation='relu')))\n",
    "    \n",
    "    #model.add(BatchNormalization())\n",
    "#     model.add(Dropout(cnn_dropout))\n",
    "    \n",
    "    model.add(TimeDistributed(Dense(cnn_output_dim, activation='relu')))\n",
    "    \n",
    "    model.add(Bidirectional(LSTM(rnn_hidden_dim, dropout=rnn_dropout, return_sequences=True), \\\n",
    "                          input_shape=(timestep, cnn_output_dim)))\n",
    "    # repeat vector for timestep\n",
    "    #model.add(RepeatVector(timestep))\n",
    "    # decode\n",
    "    model.add(Bidirectional(LSTM(rnn_hidden_dim, dropout=rnn_dropout)))\n",
    "\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    #out = rnn(cat)\n",
    "    \n",
    "    return model#Model(inputs=iL, outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model3(cnn_input_dim=21, cnn_output_dim=128, cnn_dropout=0.5,\n",
    "               rnn_hidden_dim=128, rnn_output_dim=64, num_classes=6, rnn_dropout=0.5, timestep=48,\n",
    "               bidir_mode='concat'):\n",
    "    \n",
    "    input_shape = (timestep, cnn_input_dim, cnn_input_dim, 1, )\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(TimeDistributed(Masking(), input_shape=input_shape, name='Masking'))\n",
    "    model.add(TimeDistributed(Conv2D(48, (4,4), \\\n",
    "                     padding='same', activation='relu', kernel_initializer='uniform'), \\\n",
    "                              input_shape=input_shape, name='Conv2D_1'))\n",
    "    model.add(TimeDistributed(MaxPooling2D((3,3), strides=(1,1)), name='MaxPooling2D_1'))\n",
    "    \n",
    "    model.add(TimeDistributed(Conv2D(24, (3,3), padding='same', activation='relu', kernel_initializer='uniform'),\\\n",
    "                 name='Conv2D_2'))\n",
    "    model.add(TimeDistributed(MaxPooling2D((3,3), strides=(1,1)), name='MaxPooling2D_2'))\n",
    "\n",
    "    model.add(TimeDistributed(Conv2D(12, (3,3), padding='same', activation='relu', kernel_initializer='uniform'), \\\n",
    "                             name='Conv2D_3'))\n",
    "    model.add(TimeDistributed(MaxPooling2D((3,3), strides=(1,1)), name='MaxPooling2D_3'))\n",
    "    \n",
    "    model.add(TimeDistributed(Flatten(), name='Faltten'))\n",
    "\n",
    "    model.add(TimeDistributed(Dense(128, activation='relu'), name='Dense_128'))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Bidirectional(LSTM(rnn_hidden_dim, dropout=rnn_dropout, return_sequences=True), \\\n",
    "                          input_shape=(timestep, cnn_output_dim), merge_mode=bidir_mode, name='Bi-directional_LSTM_1'))\n",
    "    model.add(Bidirectional(LSTM(rnn_hidden_dim, dropout=rnn_dropout), merge_mode=bidir_mode, name='Bi-directional_LSTM_2'))\n",
    "    model.add(Dense(6, activation='softmax', name='Output_Dense'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = init_model3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Masking (TimeDistributed)    (None, 48, 21, 21, 1)     0         \n",
      "_________________________________________________________________\n",
      "Conv2D_1 (TimeDistributed)   (None, 48, 21, 21, 48)    816       \n",
      "_________________________________________________________________\n",
      "MaxPooling2D_1 (TimeDistribu (None, 48, 19, 19, 48)    0         \n",
      "_________________________________________________________________\n",
      "Conv2D_2 (TimeDistributed)   (None, 48, 19, 19, 24)    10392     \n",
      "_________________________________________________________________\n",
      "MaxPooling2D_2 (TimeDistribu (None, 48, 17, 17, 24)    0         \n",
      "_________________________________________________________________\n",
      "Conv2D_3 (TimeDistributed)   (None, 48, 17, 17, 12)    2604      \n",
      "_________________________________________________________________\n",
      "MaxPooling2D_3 (TimeDistribu (None, 48, 15, 15, 12)    0         \n",
      "_________________________________________________________________\n",
      "Faltten (TimeDistributed)    (None, 48, 2700)          0         \n",
      "_________________________________________________________________\n",
      "Dense_128 (TimeDistributed)  (None, 48, 128)           345728    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 48, 128)           0         \n",
      "_________________________________________________________________\n",
      "Bi-directional_LSTM_1 (Bidir (None, 48, 256)           263168    \n",
      "_________________________________________________________________\n",
      "Bi-directional_LSTM_2 (Bidir (None, 256)               394240    \n",
      "_________________________________________________________________\n",
      "Output_Dense (Dense)         (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 1,018,490\n",
      "Trainable params: 1,018,490\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.Adam(lr=5*1e-4)\n",
    "model.compile(optimizer=opt,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/zguo/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "cpt = Callbacks.ModelCheckpoint(filepath='saved_models/rcnn_aug_ep100.hdf5', \n",
    "                      save_best_only=True, period=5, verbose=1)\n",
    "tensorboard = Callbacks.TensorBoard(log_dir='./logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('saved_models/rcnn_aug_ep100.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x, y, batch_size=100, epochs=60,\n",
    "          validation_split=0.2, shuffle=False, callbacks=[cpt, tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('saved_history/rcnn_aug_history.json', 'w') as outfile:  \n",
    "    json.dump(history.history, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28800 samples, validate on 7200 samples\n",
      "Epoch 1/100\n",
      "28800/28800 [==============================] - 241s 8ms/step - loss: 1.5522 - acc: 0.3018 - val_loss: 1.5422 - val_acc: 0.3571\n",
      "Epoch 2/100\n",
      "28800/28800 [==============================] - 234s 8ms/step - loss: 1.2763 - acc: 0.3983 - val_loss: 1.0378 - val_acc: 0.4697\n",
      "Epoch 3/100\n",
      "28800/28800 [==============================] - 234s 8ms/step - loss: 1.0860 - acc: 0.4692 - val_loss: 0.9510 - val_acc: 0.5094\n",
      "Epoch 4/100\n",
      "28800/28800 [==============================] - 234s 8ms/step - loss: 0.9479 - acc: 0.5215 - val_loss: 0.8956 - val_acc: 0.5414\n",
      "Epoch 5/100\n",
      "28800/28800 [==============================] - 234s 8ms/step - loss: 0.8940 - acc: 0.5459 - val_loss: 0.8682 - val_acc: 0.5553\n",
      "Epoch 6/100\n",
      "28800/28800 [==============================] - 234s 8ms/step - loss: 0.8925 - acc: 0.5502 - val_loss: 0.8542 - val_acc: 0.5601\n",
      "Epoch 7/100\n",
      "28800/28800 [==============================] - 234s 8ms/step - loss: 0.8382 - acc: 0.5653 - val_loss: 0.8373 - val_acc: 0.5831\n",
      "Epoch 8/100\n",
      "28800/28800 [==============================] - 234s 8ms/step - loss: 0.8886 - acc: 0.5549 - val_loss: 0.9552 - val_acc: 0.5097\n",
      "Epoch 9/100\n",
      "28800/28800 [==============================] - 234s 8ms/step - loss: 1.2518 - acc: 0.4326 - val_loss: 0.8813 - val_acc: 0.5385\n",
      "Epoch 10/100\n",
      "28800/28800 [==============================] - 234s 8ms/step - loss: 0.8886 - acc: 0.5532 - val_loss: 0.8360 - val_acc: 0.5714\n",
      "Epoch 11/100\n",
      "28800/28800 [==============================] - 233s 8ms/step - loss: 0.8500 - acc: 0.5644 - val_loss: 0.8521 - val_acc: 0.5406\n",
      "Epoch 12/100\n",
      "28800/28800 [==============================] - 233s 8ms/step - loss: 0.8278 - acc: 0.5731 - val_loss: 0.8102 - val_acc: 0.5944\n",
      "Epoch 13/100\n",
      "28800/28800 [==============================] - 233s 8ms/step - loss: 0.8269 - acc: 0.5797 - val_loss: 0.9676 - val_acc: 0.5569\n",
      "Epoch 14/100\n",
      "28800/28800 [==============================] - 233s 8ms/step - loss: 0.9462 - acc: 0.5474 - val_loss: 0.8015 - val_acc: 0.5681\n",
      "Epoch 15/100\n",
      "28800/28800 [==============================] - 234s 8ms/step - loss: 0.8060 - acc: 0.5955 - val_loss: 0.7770 - val_acc: 0.6314\n",
      "Epoch 16/100\n",
      "28800/28800 [==============================] - 233s 8ms/step - loss: 0.7784 - acc: 0.6143 - val_loss: 0.7346 - val_acc: 0.6529\n",
      "Epoch 17/100\n",
      "28800/28800 [==============================] - 234s 8ms/step - loss: 0.8067 - acc: 0.6025 - val_loss: 0.7315 - val_acc: 0.6565\n",
      "Epoch 18/100\n",
      "28800/28800 [==============================] - 233s 8ms/step - loss: 1.4329 - acc: 0.3717 - val_loss: 1.0274 - val_acc: 0.5149\n",
      "Epoch 19/100\n",
      "28800/28800 [==============================] - 233s 8ms/step - loss: 0.8697 - acc: 0.5821 - val_loss: 0.8407 - val_acc: 0.6029\n",
      "Epoch 20/100\n",
      "28800/28800 [==============================] - 233s 8ms/step - loss: 0.8166 - acc: 0.6002 - val_loss: 0.7613 - val_acc: 0.6331\n",
      "Epoch 21/100\n",
      "28800/28800 [==============================] - 233s 8ms/step - loss: 0.7690 - acc: 0.6254 - val_loss: 0.7511 - val_acc: 0.6350\n",
      "Epoch 22/100\n",
      "28800/28800 [==============================] - 234s 8ms/step - loss: 0.7571 - acc: 0.6308 - val_loss: 0.7346 - val_acc: 0.6385\n",
      "Epoch 23/100\n",
      "28800/28800 [==============================] - 234s 8ms/step - loss: 0.8715 - acc: 0.5934 - val_loss: 0.7296 - val_acc: 0.6443\n",
      "Epoch 24/100\n",
      "28800/28800 [==============================] - 234s 8ms/step - loss: 0.7498 - acc: 0.6375 - val_loss: 0.6947 - val_acc: 0.6637\n",
      "Epoch 25/100\n",
      "28800/28800 [==============================] - 233s 8ms/step - loss: 0.7278 - acc: 0.6440 - val_loss: 0.7035 - val_acc: 0.6654\n",
      "Epoch 26/100\n",
      "28800/28800 [==============================] - 233s 8ms/step - loss: 0.7311 - acc: 0.6448 - val_loss: 0.8608 - val_acc: 0.5956\n",
      "Epoch 27/100\n",
      "28800/28800 [==============================] - 234s 8ms/step - loss: 0.7290 - acc: 0.6495 - val_loss: 0.6876 - val_acc: 0.6781\n",
      "Epoch 28/100\n",
      "28800/28800 [==============================] - 234s 8ms/step - loss: 0.7197 - acc: 0.6548 - val_loss: 0.7100 - val_acc: 0.6640\n",
      "Epoch 29/100\n",
      "28800/28800 [==============================] - 233s 8ms/step - loss: 0.7091 - acc: 0.6617 - val_loss: 0.6421 - val_acc: 0.6910\n",
      "Epoch 30/100\n",
      "28800/28800 [==============================] - 233s 8ms/step - loss: 0.6739 - acc: 0.6764 - val_loss: 0.6526 - val_acc: 0.6744\n",
      "Epoch 31/100\n",
      "28800/28800 [==============================] - 233s 8ms/step - loss: 0.6952 - acc: 0.6672 - val_loss: 0.6727 - val_acc: 0.6793\n",
      "Epoch 32/100\n",
      "28800/28800 [==============================] - 233s 8ms/step - loss: 0.6650 - acc: 0.6798 - val_loss: 0.6509 - val_acc: 0.6867\n",
      "Epoch 33/100\n",
      "28800/28800 [==============================] - 233s 8ms/step - loss: 0.6983 - acc: 0.6664 - val_loss: 0.7184 - val_acc: 0.6706\n",
      "Epoch 34/100\n",
      "28800/28800 [==============================] - 233s 8ms/step - loss: 0.6738 - acc: 0.6825 - val_loss: 0.6964 - val_acc: 0.6856\n",
      "Epoch 35/100\n",
      "28800/28800 [==============================] - 233s 8ms/step - loss: 0.6490 - acc: 0.6933 - val_loss: 0.6327 - val_acc: 0.6942\n",
      "Epoch 36/100\n",
      " 4000/28800 [===>..........................] - ETA: 3:05 - loss: 0.6415 - acc: 0.6905"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28800/28800 [==============================] - 233s 8ms/step - loss: 0.6155 - acc: 0.7075 - val_loss: 0.6817 - val_acc: 0.6883\n",
      "Epoch 39/100\n",
      "28800/28800 [==============================] - 233s 8ms/step - loss: 0.6228 - acc: 0.7068 - val_loss: 0.5999 - val_acc: 0.7204\n",
      "Epoch 40/100\n",
      "28800/28800 [==============================] - 233s 8ms/step - loss: 0.6132 - acc: 0.7130 - val_loss: 0.6027 - val_acc: 0.7275\n",
      "Epoch 41/100\n",
      "28800/28800 [==============================] - 233s 8ms/step - loss: 0.5971 - acc: 0.7219 - val_loss: 0.5885 - val_acc: 0.7372\n",
      "Epoch 42/100\n",
      "28800/28800 [==============================] - 232s 8ms/step - loss: 0.6036 - acc: 0.7183 - val_loss: 0.5659 - val_acc: 0.7360\n",
      "Epoch 43/100\n",
      "28800/28800 [==============================] - 233s 8ms/step - loss: 0.5810 - acc: 0.7289 - val_loss: 0.5745 - val_acc: 0.7314\n",
      "Epoch 44/100\n",
      "28800/28800 [==============================] - 233s 8ms/step - loss: 0.5752 - acc: 0.7325 - val_loss: 0.5707 - val_acc: 0.7406\n",
      "Epoch 45/100\n",
      "28800/28800 [==============================] - 233s 8ms/step - loss: 0.5812 - acc: 0.7318 - val_loss: 0.5270 - val_acc: 0.7600\n",
      "Epoch 46/100\n",
      "28800/28800 [==============================] - 233s 8ms/step - loss: 0.5919 - acc: 0.7275 - val_loss: 0.5678 - val_acc: 0.7406\n",
      "Epoch 47/100\n",
      "28800/28800 [==============================] - 233s 8ms/step - loss: 0.5723 - acc: 0.7382 - val_loss: 0.5560 - val_acc: 0.7488\n",
      "Epoch 48/100\n",
      "28800/28800 [==============================] - 233s 8ms/step - loss: 0.5398 - acc: 0.7548 - val_loss: 0.5363 - val_acc: 0.7558\n",
      "Epoch 49/100\n",
      "28800/28800 [==============================] - 233s 8ms/step - loss: 0.5383 - acc: 0.7560 - val_loss: 0.5126 - val_acc: 0.7707\n",
      "Epoch 50/100\n",
      "28800/28800 [==============================] - 233s 8ms/step - loss: 0.5416 - acc: 0.7568 - val_loss: 0.5027 - val_acc: 0.7701\n",
      "Epoch 51/100\n",
      "28800/28800 [==============================] - 233s 8ms/step - loss: 0.5171 - acc: 0.7668 - val_loss: 0.5081 - val_acc: 0.7810\n",
      "Epoch 52/100\n",
      "28800/28800 [==============================] - 233s 8ms/step - loss: 0.5149 - acc: 0.7702 - val_loss: 0.5570 - val_acc: 0.7693\n",
      "Epoch 53/100\n",
      "28800/28800 [==============================] - 233s 8ms/step - loss: 0.5019 - acc: 0.7752 - val_loss: 1.1428 - val_acc: 0.6449\n",
      "Epoch 54/100\n",
      "28800/28800 [==============================] - 233s 8ms/step - loss: 0.5339 - acc: 0.7631 - val_loss: 0.5143 - val_acc: 0.7801\n",
      "Epoch 55/100\n",
      "28800/28800 [==============================] - 233s 8ms/step - loss: 0.4957 - acc: 0.7793 - val_loss: 0.4910 - val_acc: 0.7850\n",
      "Epoch 56/100\n",
      "28800/28800 [==============================] - 233s 8ms/step - loss: 0.4894 - acc: 0.7828 - val_loss: 0.4900 - val_acc: 0.7860\n",
      "Epoch 57/100\n",
      "28800/28800 [==============================] - 233s 8ms/step - loss: 0.4708 - acc: 0.7890 - val_loss: 0.4584 - val_acc: 0.8050\n",
      "Epoch 58/100\n",
      "28800/28800 [==============================] - 233s 8ms/step - loss: 0.5109 - acc: 0.7764 - val_loss: 0.4604 - val_acc: 0.8014\n",
      "Epoch 59/100\n",
      "28800/28800 [==============================] - 233s 8ms/step - loss: 0.4771 - acc: 0.7908 - val_loss: 0.4593 - val_acc: 0.7985\n",
      "Epoch 60/100\n",
      "28800/28800 [==============================] - 233s 8ms/step - loss: 0.4819 - acc: 0.7868 - val_loss: 0.4515 - val_acc: 0.8053\n",
      "Epoch 61/100\n",
      "28800/28800 [==============================] - 232s 8ms/step - loss: 0.4765 - acc: 0.7916 - val_loss: 0.4392 - val_acc: 0.8103\n",
      "Epoch 62/100\n",
      "28800/28800 [==============================] - 232s 8ms/step - loss: 0.4852 - acc: 0.7880 - val_loss: 0.4895 - val_acc: 0.7994\n",
      "Epoch 63/100\n",
      "28800/28800 [==============================] - 233s 8ms/step - loss: 0.4594 - acc: 0.7977 - val_loss: 0.4774 - val_acc: 0.8060\n",
      "Epoch 64/100\n",
      "28800/28800 [==============================] - 233s 8ms/step - loss: 0.4361 - acc: 0.8075 - val_loss: 0.4128 - val_acc: 0.8244\n",
      "Epoch 65/100\n",
      "28800/28800 [==============================] - 233s 8ms/step - loss: 0.4334 - acc: 0.8096 - val_loss: 0.3828 - val_acc: 0.8338\n",
      "Epoch 66/100\n",
      "28800/28800 [==============================] - 233s 8ms/step - loss: 0.4403 - acc: 0.8110 - val_loss: 0.4163 - val_acc: 0.8208\n",
      "Epoch 67/100\n",
      "28800/28800 [==============================] - 232s 8ms/step - loss: 0.4206 - acc: 0.8174 - val_loss: 0.4137 - val_acc: 0.8278\n",
      "Epoch 68/100\n",
      "28800/28800 [==============================] - 233s 8ms/step - loss: 0.4162 - acc: 0.8198 - val_loss: 0.4303 - val_acc: 0.8222\n",
      "Epoch 69/100\n",
      "28800/28800 [==============================] - 232s 8ms/step - loss: 0.4134 - acc: 0.8205 - val_loss: 0.3989 - val_acc: 0.8346\n",
      "Epoch 70/100\n",
      "28800/28800 [==============================] - 232s 8ms/step - loss: 0.4087 - acc: 0.8233 - val_loss: 0.4247 - val_acc: 0.8254\n",
      "Epoch 71/100\n",
      "28800/28800 [==============================] - 233s 8ms/step - loss: 0.4083 - acc: 0.8250 - val_loss: 0.4052 - val_acc: 0.8340\n",
      "Epoch 72/100\n",
      "28800/28800 [==============================] - 233s 8ms/step - loss: 0.4075 - acc: 0.8248 - val_loss: 0.3698 - val_acc: 0.8431\n",
      "Epoch 73/100\n",
      "28800/28800 [==============================] - 232s 8ms/step - loss: 0.4046 - acc: 0.8260 - val_loss: 0.4241 - val_acc: 0.8250\n",
      "Epoch 74/100\n",
      "28800/28800 [==============================] - 233s 8ms/step - loss: 0.4058 - acc: 0.8286 - val_loss: 0.3894 - val_acc: 0.8388\n",
      "Epoch 75/100\n",
      "28800/28800 [==============================] - 232s 8ms/step - loss: 0.3968 - acc: 0.8312 - val_loss: 0.3735 - val_acc: 0.8451\n",
      "Epoch 76/100\n",
      "28800/28800 [==============================] - 233s 8ms/step - loss: 0.3804 - acc: 0.8387 - val_loss: 0.3630 - val_acc: 0.8486\n",
      "Epoch 77/100\n",
      "28800/28800 [==============================] - 233s 8ms/step - loss: 0.3775 - acc: 0.8398 - val_loss: 0.3611 - val_acc: 0.8481\n",
      "Epoch 78/100\n",
      "28800/28800 [==============================] - 232s 8ms/step - loss: 0.3766 - acc: 0.8414 - val_loss: 0.3584 - val_acc: 0.8526\n",
      "Epoch 79/100\n",
      "28800/28800 [==============================] - 233s 8ms/step - loss: 0.3737 - acc: 0.8412 - val_loss: 0.3644 - val_acc: 0.8492\n",
      "Epoch 80/100\n",
      "28800/28800 [==============================] - 233s 8ms/step - loss: 0.3712 - acc: 0.8424 - val_loss: 0.3699 - val_acc: 0.8519\n",
      "Epoch 81/100\n",
      "28800/28800 [==============================] - 233s 8ms/step - loss: 0.4127 - acc: 0.8285 - val_loss: 0.3657 - val_acc: 0.8538\n",
      "Epoch 82/100\n",
      "28800/28800 [==============================] - 233s 8ms/step - loss: 0.3567 - acc: 0.8501 - val_loss: 0.3340 - val_acc: 0.8622\n",
      "Epoch 83/100\n",
      "28800/28800 [==============================] - 232s 8ms/step - loss: 0.3850 - acc: 0.8375 - val_loss: 0.3314 - val_acc: 0.8622\n",
      "Epoch 84/100\n",
      "28800/28800 [==============================] - 233s 8ms/step - loss: 0.3534 - acc: 0.8515 - val_loss: 0.3479 - val_acc: 0.8606\n",
      "Epoch 85/100\n",
      "28800/28800 [==============================] - 232s 8ms/step - loss: 0.3702 - acc: 0.8440 - val_loss: 0.3714 - val_acc: 0.8511\n",
      "Epoch 86/100\n",
      "28800/28800 [==============================] - 232s 8ms/step - loss: 0.3557 - acc: 0.8517 - val_loss: 0.3739 - val_acc: 0.8503\n",
      "Epoch 87/100\n",
      "28800/28800 [==============================] - 233s 8ms/step - loss: 0.3527 - acc: 0.8538 - val_loss: 0.4057 - val_acc: 0.8454\n",
      "Epoch 88/100\n",
      "28800/28800 [==============================] - 233s 8ms/step - loss: 0.3452 - acc: 0.8559 - val_loss: 0.3187 - val_acc: 0.8722\n",
      "Epoch 89/100\n",
      "28800/28800 [==============================] - 233s 8ms/step - loss: 0.3843 - acc: 0.8381 - val_loss: 0.3703 - val_acc: 0.8594\n",
      "Epoch 90/100\n",
      "28800/28800 [==============================] - 232s 8ms/step - loss: 0.3445 - acc: 0.8557 - val_loss: 0.3121 - val_acc: 0.8736\n",
      "Epoch 91/100\n",
      "28800/28800 [==============================] - 233s 8ms/step - loss: 0.3344 - acc: 0.8593 - val_loss: 0.3364 - val_acc: 0.8692\n",
      "Epoch 92/100\n",
      "28800/28800 [==============================] - 233s 8ms/step - loss: 0.3445 - acc: 0.8579 - val_loss: 0.3113 - val_acc: 0.8753\n",
      "Epoch 93/100\n",
      "28800/28800 [==============================] - 232s 8ms/step - loss: 0.3352 - acc: 0.8600 - val_loss: 0.3048 - val_acc: 0.8796\n",
      "Epoch 94/100\n",
      "28800/28800 [==============================] - 233s 8ms/step - loss: 0.3266 - acc: 0.8652 - val_loss: 0.3698 - val_acc: 0.8582\n",
      "Epoch 95/100\n",
      "28800/28800 [==============================] - 233s 8ms/step - loss: 0.3334 - acc: 0.8631 - val_loss: 0.3486 - val_acc: 0.8703\n",
      "Epoch 96/100\n",
      "28800/28800 [==============================] - 232s 8ms/step - loss: 0.3310 - acc: 0.8611 - val_loss: 0.3118 - val_acc: 0.8758\n",
      "Epoch 97/100\n",
      "28800/28800 [==============================] - 233s 8ms/step - loss: 0.3304 - acc: 0.8648 - val_loss: 0.3321 - val_acc: 0.8733\n",
      "Epoch 98/100\n",
      "28800/28800 [==============================] - 232s 8ms/step - loss: 0.3209 - acc: 0.8689 - val_loss: 0.3036 - val_acc: 0.8812\n",
      "Epoch 99/100\n",
      "28800/28800 [==============================] - 232s 8ms/step - loss: 0.3153 - acc: 0.8702 - val_loss: 0.3300 - val_acc: 0.8761\n",
      "Epoch 100/100\n",
      "28800/28800 [==============================] - 232s 8ms/step - loss: 0.3130 - acc: 0.8720 - val_loss: 0.3266 - val_acc: 0.8733\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8e47b13cc0>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x[::2], y[::2], batch_size=80, epochs=100,\n",
    "              validation_split=0.2, shuffle=True, callbacks=[cpt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = Callbacks.TensorBoard(log_dir='./logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
